{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a7a2d0-cdb5-417e-8ff3-e198883e1cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+\n|Customer ID|   Customer Name|\n+-----------+----------------+\n|   ID135600| Susan Hernandez|\n|   ID135601|   Jackie Wagner|\n|   ID135602|    Mariah Mccoy|\n|   ID135603|  Courtney Doyle|\n|   ID135604|     Dawn Torres|\n|   ID135605|Jessica Faulkner|\n|   ID135606|   Alicia Howard|\n|   ID135607|      David Hill|\n|   ID135608| Matthew Johnson|\n|   ID135609| Kimberly Murphy|\n+-----------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# loading customer file from AWS s3\n",
    "\n",
    "df_load_cust = spark.read.csv(\"s3://amazon-l0-landing-prod/landing/customer/customer.csv\", header=True, inferSchema=True)\n",
    "df_load_cust = df_load_cust.drop(\"Segment\")\n",
    "\n",
    "df_load_cust.createOrReplaceTempView(\"df_load_cust\")\n",
    "\n",
    "spark.sql(\"select * from df_load_cust limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e578c963-b3be-41aa-b7c7-20f8ae20fddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+--------------------+\n| cust_id|           cust_nm|             load_dt|\n+--------+------------------+--------------------+\n|ID135600|   Susan Hernandez|2025-11-10 02:19:...|\n|ID135601|     Jackie Wagner|2025-11-10 02:19:...|\n|ID135602|      Mariah Mccoy|2025-11-10 02:19:...|\n|ID135603|    Courtney Doyle|2025-11-10 02:19:...|\n|ID135604|       Dawn Torres|2025-11-10 02:19:...|\n|ID135605|  Jessica Faulkner|2025-11-10 02:19:...|\n|ID135606|     Alicia Howard|2025-11-10 02:19:...|\n|ID135607|        David Hill|2025-11-10 02:19:...|\n|ID135608|   Matthew Johnson|2025-11-10 02:19:...|\n|ID135609|   Kimberly Murphy|2025-11-10 02:19:...|\n|ID135610|       Tyler Brown|2025-11-10 02:19:...|\n|ID135611|    Sheila Coleman|2025-11-10 02:19:...|\n|ID135612| Douglas Rodriguez|2025-11-10 02:19:...|\n|ID135613|       Brandi King|2025-11-10 02:19:...|\n|ID135614|      Susan Brewer|2025-11-10 02:19:...|\n|ID135615|     Latoya Wright|2025-11-10 02:19:...|\n|ID135616|   Richard Edwards|2025-11-10 02:19:...|\n|ID135617|Mr. Michael Taylor|2025-11-10 02:19:...|\n|ID135618|     Angela Castro|2025-11-10 02:19:...|\n|ID135619|    Christie Brown|2025-11-10 02:19:...|\n+--------+------------------+--------------------+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Restructuring the file\n",
    "\n",
    "# Renaming headers\n",
    "df_restructured_cust = df_load_cust.withColumnRenamed(\"Customer ID\", \"cust_id\").withColumnRenamed(\"Customer Name\", \"cust_nm\")\n",
    "\n",
    "df_restructured_cust.createOrReplaceTempView(\"df_restructured_cust\")\n",
    "\n",
    "# Renaming segment values\n",
    "# Adding timestamp\n",
    "df_renamed_cust = spark.sql(\"\"\"\n",
    "                        select\n",
    "                        cust_id, \n",
    "                        cust_nm,  \n",
    "                        now() as load_dt\n",
    "                        from df_restructured_cust\n",
    "                        \"\"\")\n",
    "\n",
    "df_renamed_cust.createOrReplaceTempView(\"df_renamed_cust\")\n",
    "\n",
    "df_renamed_cust.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f111c60-1d2c-4da3-b55c-b95a71b24fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining s3 write path\n",
    "s3_path = \"s3://amazon-l1-staging-prod/staging/\"+\"customer/\"\n",
    "#print (s3_path)\n",
    "\n",
    "# Write parquet file to s3\n",
    "\n",
    "df_renamed_cust.write.parquet(s3_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "d_cust",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}